import re
import unicodedata
from typing import Dict, List, Optional
from datetime import datetime, timedelta

class DataCleaner:
    def __init__(self):
        # åˆå§‹åŒ–åŒä¹‰è¯è¡¨
        self.synonym_dict = {
            'APP': 'åº”ç”¨',
            'app': 'åº”ç”¨',
            'å®¢æœå°å§å§': 'å®¢æœäººå‘˜',
            'é€€æ¢è´§': 'é€€è´§/æ¢è´§',
            'äº²': 'æ‚¨å¥½',
            'äº²äº²': 'æ‚¨å¥½'
        }
        
        # åˆå§‹åŒ–æ•æ„Ÿè¯åˆ—è¡¨
        self.forbidden_words = [
            "å‚»é€¼", "ä»–å¦ˆçš„", "æ“", "æ»š", "ç™½ç—´", "ç¬¨è›‹"
        ]
        
        # åˆå§‹åŒ–æµ‹è¯•å…³é”®è¯
        self.test_keywords = [
            "test", "æµ‹è¯•", "æµ‹è¯•æ•°æ®", "æµ‹è¯•ç”¨ä¾‹", "æµ‹è¯•ç¯å¢ƒ"
        ]
        
        # åˆå§‹åŒ–è¯­å¢ƒå…³é”®è¯æ˜ å°„
        self.context_keywords = {
            'æ”¯ä»˜é—®é¢˜': ['æ”¯ä»˜', 'ä»˜æ¬¾', 'é€€æ¬¾', 'è½¬è´¦', 'ä½™é¢', 'è´¹ç”¨', 'æ”¶è´¹', 'ä»·æ ¼', 'é‡‘é¢', 'æ”¯ä»˜æ–¹å¼', 'æ”¯ä»˜å®', 'å¾®ä¿¡æ”¯ä»˜'],
            'ç‰©æµé—®é¢˜': ['å¿«é€’', 'ç‰©æµ', 'é…é€', 'å‘è´§', 'æ”¶è´§', 'è¿è¾“', 'é€è¾¾', 'æ´¾é€', 'åŒ…è£¹', 'å¿«é€’è´¹', 'è¿è´¹'],
            'è´¦æˆ·é—®é¢˜': ['ç™»å½•', 'æ³¨å†Œ', 'å¯†ç ', 'è´¦å·', 'è®¤è¯', 'å®å', 'èº«ä»½', 'ç»‘å®š', 'è§£ç»‘', 'æ³¨é”€', 'æŒ‚å¤±', 'æ‰¾å›'],
            'å•†å“é—®é¢˜': ['å•†å“', 'äº§å“', 'ä»·æ ¼', 'è´¨é‡', 'è§„æ ¼', 'å‹å·', 'å“ç‰Œ', 'åº“å­˜', 'ç¼ºè´§', 'æ–­è´§', 'ä¸‹æ¶', 'ä¸Šæ¶'],
            'è®¢å•é—®é¢˜': ['è®¢å•', 'ä¸‹å•', 'å–æ¶ˆ', 'ä¿®æ”¹', 'æŸ¥è¯¢', 'è·Ÿè¸ª', 'çŠ¶æ€', 'è¿›åº¦', 'ç¡®è®¤', 'å–æ¶ˆ', 'é€€æ¬¾', 'é€€è´§'],
            'æœåŠ¡é—®é¢˜': ['æœåŠ¡', 'å®¢æœ', 'å”®å', 'ç»´ä¿®', 'ä¿ä¿®', 'ä¿å…»', 'å®‰è£…', 'è°ƒè¯•', 'åŸ¹è®­', 'æŒ‡å¯¼', 'å’¨è¯¢'],
            'ç³»ç»Ÿé—®é¢˜': ['ç³»ç»Ÿ', 'è½¯ä»¶', 'ç¨‹åº', 'åº”ç”¨', 'APP', 'ç½‘é¡µ', 'ç½‘ç«™', 'ç½‘ç»œ', 'è¿æ¥', 'å¡é¡¿', 'å´©æºƒ', 'é”™è¯¯'],
            'å®‰å…¨éšç§': ['å®‰å…¨', 'éšç§', 'ä¿æŠ¤', 'æ³„éœ²', 'åŠ å¯†', 'è§£å¯†', 'æˆæƒ', 'æƒé™', 'éªŒè¯', 'è®¤è¯', 'å®å'],
            'ä¼˜æƒ æ´»åŠ¨': ['ä¼˜æƒ ', 'æ´»åŠ¨', 'ä¿ƒé”€', 'æŠ˜æ‰£', 'æ»¡å‡', 'åˆ¸', 'çº¢åŒ…', 'ç§¯åˆ†', 'ä¼šå‘˜', 'VIP', 'ç‰¹æƒ'],
            'æŠ•è¯‰å»ºè®®': ['æŠ•è¯‰', 'å»ºè®®', 'åé¦ˆ', 'æ„è§', 'ä¸¾æŠ¥', 'ç»´æƒ', 'çº çº·', 'äº‰è®®', 'é—®é¢˜', 'ä¸æ»¡', 'å·®è¯„'],
            'ETCä¸šåŠ¡': ['etc', 'é«˜é€Ÿå…¬è·¯', 'é€šè¡Œè´¹', 'æ”¶è´¹ç«™', 'etcå¡', 'etcè®¾å¤‡'],
            'è½¦è¾†ç›¸å…³': ['è½¦ç‰Œ', 'è½¦è¾†', 'æ±½è½¦', 'é©¾ç…§', 'é©¾é©¶è¯', 'è¡Œé©¶è¯', 'è¿ç« ', 'å¹´æ£€', 'ä¿é™©'],
            'ç¥¨æ®é—®é¢˜': ['å‘ç¥¨', 'ç¥¨æ®', 'æ”¶æ®', 'æŠ¥é”€', 'å‡­è¯', 'å•æ®'],
            'ç´§æ€¥é—®é¢˜': ['ç´§æ€¥', 'æ€¥', 'å¿«', 'ç«‹å³', 'é©¬ä¸Š', 'å°½å¿«', 'åŠ æ€¥', 'åŠ æ€¥å¤„ç†']
        }
        
        # åˆå§‹åŒ–æ— æ•ˆå†…å®¹å…³é”®è¯
        self.invalid_keywords = [
            # ç³»ç»Ÿæ¶ˆæ¯å’Œæ ‡è®°
            "[å›¾ç‰‡]", "[è¡¨æƒ…]", "[è¯­éŸ³]", "[è§†é¢‘]", "[æ–‡ä»¶]",
            "[é“¾æ¥]", "[çº¢åŒ…]", "[è½¬è´¦]", "[ä½ç½®]", "[åç‰‡]",
            "[å°ç¨‹åº]", "[å…¬ä¼—å·]", "[ç¾¤èŠ]", "[ç§èŠ]", "[ç³»ç»Ÿ]",
            
            # è‡ªåŠ¨å›å¤å’Œæ¨¡æ¿æ¶ˆæ¯
            "æ‚¨å¥½ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡",
            "æ„Ÿè°¢æ‚¨çš„å’¨è¯¢",
            "æ­£åœ¨ä¸ºæ‚¨è½¬æ¥",
            "è¯·ç¨ç­‰",
            "æ­£åœ¨å¤„ç†ä¸­",
            "æ­£åœ¨ä¸ºæ‚¨æŸ¥è¯¢",
            "æ­£åœ¨ä¸ºæ‚¨æ ¸å®",
            "æ­£åœ¨ä¸ºæ‚¨å¤„ç†",
            "æ­£åœ¨ä¸ºæ‚¨åŠç†",
            "æ­£åœ¨ä¸ºæ‚¨å®¡æ ¸",
            
            # å¸¸è§æ— æ•ˆå†…å®¹
            "æ”¶åˆ°", "å¥½çš„", "å—¯", "å“¦", "å•Š", "å‘€", "å‘¢", "å§",
            "è°¢è°¢", "ä¸å®¢æ°”", "å†è§", "æ‹œæ‹œ", "æ™šå®‰", "æ—©å®‰",
            "åœ¨å—", "åœ¨çš„", "åœ¨çš„å“¦", "åœ¨çš„å‘¢", "åœ¨çš„å‘€",
            "è¯·ç¨ç­‰", "ç¨ç­‰", "ç­‰ç­‰", "ç­‰ä¸€ä¸‹", "ç¨ç­‰ç‰‡åˆ»",
            "æ­£åœ¨è¾“å…¥", "æ­£åœ¨è¾“å…¥ä¸­", "å¯¹æ–¹æ­£åœ¨è¾“å…¥",
            "å·²è¯»", "å·²é€è¾¾", "å·²å‘é€", "å‘é€æˆåŠŸ",
            "æ­£åœ¨åŠ è½½", "åŠ è½½ä¸­", "åŠ è½½å¤±è´¥", "åŠ è½½å®Œæˆ",
            
            # è¡¨æƒ…ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
            "ğŸ˜Š", "ğŸ˜„", "ğŸ˜ƒ", "ğŸ˜€", "ğŸ˜", "ğŸ˜…", "ğŸ˜‚", "ğŸ¤£",
            "ğŸ‘", "ğŸ‘", "ğŸ‘Œ", "âœŒï¸", "ğŸ¤", "ğŸ™", "ğŸ’ª",
            "â¤ï¸", "ğŸ’•", "ğŸ’–", "ğŸ’—", "ğŸ’“", "ğŸ’", "ğŸ’",
            "ğŸŒŸ", "â­", "âœ¨", "ğŸ’«", "ğŸ’¥", "ğŸ’¦", "ğŸ’¨",
            
            # å¹¿å‘Šå’Œæ¨å¹¿
            "æ¨å¹¿", "å¹¿å‘Š", "ä¼˜æƒ ", "ä¿ƒé”€", "ç‰¹ä»·", "æŠ˜æ‰£",
            "é™æ—¶", "ç§’æ€", "æŠ¢è´­", "å›¢è´­", "æ‹¼å›¢", "ç ä»·",
            "æŠ½å¥–", "ä¸­å¥–", "å¥–å“", "ç¤¼å“", "èµ å“", "ç¤¼åŒ…",
            
            # ç³»ç»Ÿæç¤ºå’ŒçŠ¶æ€
            "ç³»ç»Ÿç»´æŠ¤ä¸­", "ç³»ç»Ÿå‡çº§ä¸­", "ç³»ç»Ÿæ›´æ–°ä¸­",
            "ç½‘ç»œè¿æ¥ä¸­", "æ­£åœ¨è¿æ¥", "è¿æ¥å¤±è´¥",
            "æ­£åœ¨åŒæ­¥", "åŒæ­¥å®Œæˆ", "åŒæ­¥å¤±è´¥",
            "æ­£åœ¨ä¸‹è½½", "ä¸‹è½½å®Œæˆ", "ä¸‹è½½å¤±è´¥",
            "æ­£åœ¨ä¸Šä¼ ", "ä¸Šä¼ å®Œæˆ", "ä¸Šä¼ å¤±è´¥",
            
            # å…¶ä»–æ— æ•ˆå†…å®¹
            "ç‚¹å‡»æŸ¥çœ‹", "æŸ¥çœ‹æ›´å¤š", "æŸ¥çœ‹è¯¦æƒ…",
            "å¤åˆ¶æˆåŠŸ", "å¤åˆ¶å¤±è´¥", "å¤åˆ¶é“¾æ¥",
            "åˆ†äº«æˆåŠŸ", "åˆ†äº«å¤±è´¥", "åˆ†äº«é“¾æ¥",
            "è½¬å‘æˆåŠŸ", "è½¬å‘å¤±è´¥", "è½¬å‘é“¾æ¥",
            "ä¿å­˜æˆåŠŸ", "ä¿å­˜å¤±è´¥", "ä¿å­˜å›¾ç‰‡",
            "åˆ é™¤æˆåŠŸ", "åˆ é™¤å¤±è´¥", "åˆ é™¤æ¶ˆæ¯",
            "æ’¤å›æˆåŠŸ", "æ’¤å›å¤±è´¥", "æ’¤å›æ¶ˆæ¯",
            "æ¸…ç©ºæˆåŠŸ", "æ¸…ç©ºå¤±è´¥", "æ¸…ç©ºèŠå¤©",
            "ç½®é¡¶æˆåŠŸ", "ç½®é¡¶å¤±è´¥", "å–æ¶ˆç½®é¡¶",
            "å…æ‰“æ‰°", "æ¶ˆæ¯å…æ‰“æ‰°", "ç¾¤èŠå…æ‰“æ‰°",
            "å·²é™éŸ³", "å·²å±è”½", "å·²æ‹‰é»‘", "å·²ä¸¾æŠ¥",
            "å·²å…³æ³¨", "å·²å–æ¶ˆå…³æ³¨", "å·²ç‚¹èµ", "å·²æ”¶è—"
        ]

        # æ·»åŠ å…³é”®è¯æƒé‡
        self.keyword_weights = {
            'æ”¯ä»˜é—®é¢˜': 1.2,
            'ç‰©æµé—®é¢˜': 1.1,
            'è´¦æˆ·é—®é¢˜': 1.3,
            'å•†å“é—®é¢˜': 1.0,
            'è®¢å•é—®é¢˜': 1.2,
            'æœåŠ¡é—®é¢˜': 1.1,
            'ç³»ç»Ÿé—®é¢˜': 1.0,
            'å®‰å…¨éšç§': 1.3,
            'ä¼˜æƒ æ´»åŠ¨': 0.8,
            'æŠ•è¯‰å»ºè®®': 1.2,
            'ETCä¸šåŠ¡': 1.4,
            'è½¦è¾†ç›¸å…³': 1.2,
            'ç¥¨æ®é—®é¢˜': 1.1,
            'ç´§æ€¥é—®é¢˜': 1.5
        }
        
        # æ·»åŠ å…³é”®è¯é‡è¦æ€§æƒé‡
        self.importance_weights = {
            'etc': 2.0,
            'æ”¯ä»˜': 1.8,
            'é€€æ¬¾': 1.8,
            'è´¦æˆ·': 1.7,
            'å¯†ç ': 1.7,
            'å®‰å…¨': 1.6,
            'ç´§æ€¥': 1.6,
            'æ•…éšœ': 1.5,
            'é”™è¯¯': 1.5,
            'é—®é¢˜': 1.4,
            'å¼‚å¸¸': 1.4,
            'æ— æ³•': 1.4,
            'ä¸èƒ½': 1.4,
            'æ€ä¹ˆåŠ': 1.3,
            'å¦‚ä½•': 1.3,
            'æ€ä¹ˆ': 1.3,
            'ä¸ºä»€ä¹ˆ': 1.3,
            'åŸå› ': 1.3,
            'è§£å†³': 1.3
        }

    def standardize_encoding(self, text: str) -> str:
        """ç»Ÿä¸€ç¼–ç æ ¼å¼"""
        if not isinstance(text, str):
            return str(text)
        return unicodedata.normalize('NFKC', text)

    def standardize_datetime(self, dt: datetime) -> datetime:
        """æ ‡å‡†åŒ–æ—¥æœŸæ—¶é—´æ ¼å¼"""
        if not isinstance(dt, datetime):
            return dt
        return dt.replace(microsecond=0)

    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬å†…å®¹"""
        if not isinstance(text, str):
            return str(text)
        
        # 1. ç»Ÿä¸€ç¼–ç 
        text = self.standardize_encoding(text)
        
        # 2. åˆ é™¤ç³»ç»Ÿæ ‡è®°å’Œç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'\[.*?\]', '', text)  # åˆ é™¤æ–¹æ‹¬å·å†…å®¹
        text = re.sub(r'ã€.*?ã€‘', '', text)  # åˆ é™¤ä¸­æ–‡æ–¹æ‹¬å·å†…å®¹
        text = re.sub(r'<.*?>', '', text)   # åˆ é™¤HTMLæ ‡ç­¾
        
        # 3. åˆ é™¤é“¾æ¥
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)  # åˆ é™¤httpé“¾æ¥
        text = re.sub(r'www\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)  # åˆ é™¤wwwé“¾æ¥
        text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', '', text)  # åˆ é™¤é‚®ç®±é“¾æ¥
        text = re.sub(r'[a-zA-Z0-9-]+(?:\.[a-zA-Z0-9-]+)*\.[a-zA-Z]{2,}', '', text)  # åˆ é™¤åŸŸå
        text = re.sub(r'[a-zA-Z0-9-]+(?:\.[a-zA-Z0-9-]+)*\.(?:com|cn|net|org|edu|gov|mil|biz|info|name|mobi|asia|app|dev|io|co|me|tv|cc|xyz|top|site|online|tech|store|blog|shop|club|fun|game|live|news|video|music|photo|cloud|host|link|network|services|solutions|agency|studio|design|digital|media|marketing|agency|consulting|group|inc|ltd|llc|corp|company|business|enterprise|solutions|services|agency|studio|design|digital|media|marketing|agency|consulting|group|inc|ltd|llc|corp|company|business|enterprise)', '', text)  # åˆ é™¤å¸¸è§åŸŸååç¼€
        
        # 4. åªä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’ŒåŸºæœ¬æ ‡ç‚¹
        text = re.sub(r'[^\w\s\u4e00-\u9fffï¼Œã€‚ï¼ï¼Ÿã€ï¼šï¼›""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹]', '', text)
        
        # 5. åˆ é™¤é‡å¤å­—ç¬¦å’Œå¤šä½™ç©ºæ ¼
        text = re.sub(r'(.)\1{2,}', r'\1\1', text)  # ç¼©å‡é‡å¤å­—ç¬¦
        text = re.sub(r'\s+', ' ', text)  # åˆå¹¶å¤šä¸ªç©ºæ ¼
        
        # 6. æ›¿æ¢åŒä¹‰è¯å’Œæ ‡å‡†åŒ–ç”¨è¯­
        for old, new in self.synonym_dict.items():
            text = text.replace(old, new)
        
        # 7. è„±æ•å¤„ç†
        text = re.sub(r'(1[3-9]\d{9})', '*******\g<1>[-4:]', text)  # æ‰‹æœºå·
        text = re.sub(r'(\d{17}[\dXx])', 'ID_\g<1>[-4:]', text)  # èº«ä»½è¯å·
        text = re.sub(r'(\d{16})', 'CARD_\g<1>[-4:]', text)  # é“¶è¡Œå¡å·
        
        # 8. åˆ é™¤æ— æ•ˆå†…å®¹
        for keyword in self.invalid_keywords:
            text = text.replace(keyword, '')
            
        # 9. åˆ é™¤è¿‡çŸ­çš„å¥å­å’Œæ— æ„ä¹‰å†…å®¹
        text = re.sub(r'^[\s\.,ï¼Œã€‚!ï¼?ï¼Ÿ]+$', '', text)  # åˆ é™¤åªæœ‰æ ‡ç‚¹çš„å¥å­
        text = re.sub(r'^[a-zA-Z0-9\s]+$', '', text)  # åˆ é™¤çº¯è‹±æ–‡æ•°å­—çš„å¥å­
        
        # 10. æ ‡å‡†åŒ–æ ‡ç‚¹ç¬¦å·
        text = text.replace('ï¼Œ', ',')
        text = text.replace('ã€‚', '.')
        text = text.replace('ï¼', '!')
        text = text.replace('ï¼Ÿ', '?')
        text = text.replace('ï¼š', ':')
        text = text.replace('ï¼›', ';')
        
        # 11. åˆ é™¤å¤šä½™çš„ç©ºæ ¼å’Œæ¢è¡Œ
        text = re.sub(r'\n+', ' ', text)  # å°†æ¢è¡Œæ›¿æ¢ä¸ºç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)  # åˆå¹¶å¤šä¸ªç©ºæ ¼
        text = text.strip()  # åˆ é™¤é¦–å°¾ç©ºæ ¼
        
        return text

    def is_valid_message(self, text: str) -> bool:
        """åˆ¤æ–­æ¶ˆæ¯æ˜¯å¦æœ‰æ•ˆ"""
        if not isinstance(text, str):
            return False
            
        # 1. æ£€æŸ¥é•¿åº¦
        if len(text.strip()) < 3:
            return False
            
        # 2. æ£€æŸ¥æ˜¯å¦åŒ…å«æµ‹è¯•å…³é”®è¯
        if any(keyword in text.lower() for keyword in self.test_keywords):
            return False
            
        # 3. æ£€æŸ¥æ˜¯å¦åŒ…å«æ•æ„Ÿè¯
        if any(word in text for word in self.forbidden_words):
            return False
            
        # 4. æ£€æŸ¥æ˜¯å¦åªåŒ…å«æ ‡ç‚¹ç¬¦å·
        if re.match(r'^[\s\.,ï¼Œã€‚!ï¼?ï¼Ÿ]+$', text):
            return False
            
        # 5. æ£€æŸ¥æ˜¯å¦åªåŒ…å«è‹±æ–‡æ•°å­—
        if re.match(r'^[a-zA-Z0-9\s]+$', text):
            return False
            
        # 6. æ£€æŸ¥æ˜¯å¦åŒ…å«ç³»ç»Ÿæ¶ˆæ¯ç‰¹å¾
        if re.search(r'ç³»ç»Ÿ|æç¤º|é€šçŸ¥|å…¬å‘Š|æ¶ˆæ¯|æé†’', text):
            return False
            
        # 7. æ£€æŸ¥æ˜¯å¦åŒ…å«è‡ªåŠ¨å›å¤ç‰¹å¾
        if re.search(r'æ‚¨å¥½|æ„Ÿè°¢|è°¢è°¢|å†è§|æ¬¢è¿|å®¢æœ', text) and len(text) < 10:
            return False
            
        return True

    def extract_tags(self, text: str) -> List[str]:
        """æå–æ–‡æœ¬æ ‡ç­¾"""
        tags = []
        
        # å®šä¹‰æ ‡ç­¾è§„åˆ™
        tag_rules = {
            'æ”¯ä»˜é—®é¢˜': r'æ”¯ä»˜|ä»˜æ¬¾|é€€æ¬¾|è½¬è´¦|ä½™é¢|è´¹ç”¨|æ”¶è´¹|ä»·æ ¼|é‡‘é¢|æ”¯ä»˜æ–¹å¼|æ”¯ä»˜å®|å¾®ä¿¡æ”¯ä»˜',
            'ç‰©æµé—®é¢˜': r'å¿«é€’|ç‰©æµ|é…é€|å‘è´§|æ”¶è´§|è¿è¾“|é€è¾¾|æ´¾é€|åŒ…è£¹|å¿«é€’è´¹|è¿è´¹',
            'è´¦æˆ·é—®é¢˜': r'ç™»å½•|æ³¨å†Œ|å¯†ç |è´¦å·|è®¤è¯|å®å|èº«ä»½|ç»‘å®š|è§£ç»‘|æ³¨é”€|æŒ‚å¤±|æ‰¾å›',
            'å•†å“é—®é¢˜': r'å•†å“|äº§å“|ä»·æ ¼|è´¨é‡|è§„æ ¼|å‹å·|å“ç‰Œ|åº“å­˜|ç¼ºè´§|æ–­è´§|ä¸‹æ¶|ä¸Šæ¶',
            'è®¢å•é—®é¢˜': r'è®¢å•|ä¸‹å•|å–æ¶ˆ|ä¿®æ”¹|æŸ¥è¯¢|è·Ÿè¸ª|çŠ¶æ€|è¿›åº¦|ç¡®è®¤|å–æ¶ˆ|é€€æ¬¾|é€€è´§',
            'æœåŠ¡é—®é¢˜': r'æœåŠ¡|å®¢æœ|å”®å|ç»´ä¿®|ä¿ä¿®|ä¿å…»|å®‰è£…|è°ƒè¯•|åŸ¹è®­|æŒ‡å¯¼|å’¨è¯¢',
            'ç³»ç»Ÿé—®é¢˜': r'ç³»ç»Ÿ|è½¯ä»¶|ç¨‹åº|åº”ç”¨|APP|ç½‘é¡µ|ç½‘ç«™|ç½‘ç»œ|è¿æ¥|å¡é¡¿|å´©æºƒ|é”™è¯¯',
            'å®‰å…¨éšç§': r'å®‰å…¨|éšç§|ä¿æŠ¤|æ³„éœ²|åŠ å¯†|è§£å¯†|æˆæƒ|æƒé™|éªŒè¯|è®¤è¯|å®å',
            'ä¼˜æƒ æ´»åŠ¨': r'ä¼˜æƒ |æ´»åŠ¨|ä¿ƒé”€|æŠ˜æ‰£|æ»¡å‡|åˆ¸|çº¢åŒ…|ç§¯åˆ†|ä¼šå‘˜|VIP|ç‰¹æƒ',
            'æŠ•è¯‰å»ºè®®': r'æŠ•è¯‰|å»ºè®®|åé¦ˆ|æ„è§|ä¸¾æŠ¥|ç»´æƒ|çº çº·|äº‰è®®|é—®é¢˜|ä¸æ»¡|å·®è¯„'
        }
        
        # è½¬æ¢ä¸ºå°å†™ä»¥è¿›è¡Œä¸åŒºåˆ†å¤§å°å†™çš„åŒ¹é…
        text = text.lower()
        
        for tag, pattern in tag_rules.items():
            if re.search(pattern, text):
                tags.append(tag)
        
        # æ·»åŠ ç‰¹æ®Šæ ‡ç­¾
        if re.search(r'etc|é«˜é€Ÿå…¬è·¯|é€šè¡Œè´¹|æ”¶è´¹ç«™|etcå¡|etcè®¾å¤‡', text):
            tags.append('ETCä¸šåŠ¡')
            
        if re.search(r'è½¦ç‰Œ|è½¦è¾†|æ±½è½¦|é©¾ç…§|é©¾é©¶è¯|è¡Œé©¶è¯|è¿ç« |å¹´æ£€|ä¿é™©', text):
            tags.append('è½¦è¾†ç›¸å…³')
            
        if re.search(r'å‘ç¥¨|ç¥¨æ®|æ”¶æ®|æŠ¥é”€|å‡­è¯|å•æ®', text):
            tags.append('ç¥¨æ®é—®é¢˜')
            
        if re.search(r'ç´§æ€¥|æ€¥|å¿«|ç«‹å³|é©¬ä¸Š|å°½å¿«|åŠ æ€¥|åŠ æ€¥å¤„ç†', text):
            tags.append('ç´§æ€¥é—®é¢˜')
            
        # å»é‡
        return list(set(tags))

    def is_new_session(self, current_time: datetime, last_time: datetime, 
                      timeout_minutes: int = 30) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ–°ä¼šè¯"""
        if not isinstance(current_time, datetime) or not isinstance(last_time, datetime):
            return True
        time_diff = (current_time - last_time).total_seconds() / 60
        return time_diff > timeout_minutes 

    def calculate_semantic_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬çš„è¯­ä¹‰ç›¸ä¼¼åº¦"""
        if not text1 or not text2:
            return 0.0
            
        # 1. åˆ†è¯å¹¶è®¡ç®—è¯é¢‘
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        # 2. è®¡ç®—Jaccardç›¸ä¼¼åº¦
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        if union == 0:
            return 0.0
        jaccard = intersection / union
        
        # 3. è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
        keyword_matches = 0
        total_keywords = 0
        
        for word in words1:
            if word in self.importance_weights:
                total_keywords += 1
                if word in words2:
                    keyword_matches += 1
                    
        for word in words2:
            if word in self.importance_weights:
                total_keywords += 1
                if word in words1:
                    keyword_matches += 1
                    
        keyword_similarity = keyword_matches / total_keywords if total_keywords > 0 else 0.0
        
        # 4. ç»¼åˆè®¡ç®—ç›¸ä¼¼åº¦
        return 0.6 * jaccard + 0.4 * keyword_similarity

    def calculate_context_match(self, question: str, answer: str) -> float:
        """è®¡ç®—é—®é¢˜å’Œå›ç­”çš„è¯­å¢ƒåŒ¹é…åº¦"""
        if not question or not answer:
            return 0.0
            
        # 1. è·å–é—®é¢˜å’Œå›ç­”çš„æ ‡ç­¾
        question_tags = self.extract_tags(question)
        answer_tags = self.extract_tags(answer)
        
        # 2. è®¡ç®—æ ‡ç­¾åŒ¹é…åº¦
        tag_match_score = 0.0
        if question_tags and answer_tags:
            matching_tags = set(question_tags) & set(answer_tags)
            if matching_tags:
                # è®¡ç®—åŠ æƒæ ‡ç­¾åŒ¹é…åº¦
                total_weight = 0
                matched_weight = 0
                
                for tag in question_tags:
                    total_weight += self.keyword_weights.get(tag, 1.0)
                    if tag in matching_tags:
                        matched_weight += self.keyword_weights.get(tag, 1.0)
                        
                for tag in answer_tags:
                    total_weight += self.keyword_weights.get(tag, 1.0)
                    if tag in matching_tags:
                        matched_weight += self.keyword_weights.get(tag, 1.0)
                        
                tag_match_score = matched_weight / total_weight if total_weight > 0 else 0.0
        
        # 3. è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
        keyword_match_score = 0.0
        question_keywords = set()
        answer_keywords = set()
        
        for tag in matching_tags:
            if tag in self.context_keywords:
                question_keywords.update(self.context_keywords[tag])
                answer_keywords.update(self.context_keywords[tag])
        
        if question_keywords and answer_keywords:
            # è®¡ç®—åŠ æƒå…³é”®è¯åŒ¹é…åº¦
            question_count = sum(self.importance_weights.get(keyword, 1.0) 
                               for keyword in question_keywords 
                               if keyword in question.lower())
            answer_count = sum(self.importance_weights.get(keyword, 1.0) 
                             for keyword in answer_keywords 
                             if keyword in answer.lower())
            
            total_keywords = len(question_keywords) + len(answer_keywords)
            if total_keywords > 0:
                keyword_match_score = (question_count + answer_count) / total_keywords
        
        # 4. è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
        semantic_score = self.calculate_semantic_similarity(question, answer)
        
        # 5. ç»¼åˆè®¡ç®—æœ€ç»ˆåŒ¹é…åº¦
        final_score = (
            0.4 * tag_match_score +      # æ ‡ç­¾åŒ¹é…æƒé‡
            0.3 * keyword_match_score +  # å…³é”®è¯åŒ¹é…æƒé‡
            0.3 * semantic_score         # è¯­ä¹‰ç›¸ä¼¼åº¦æƒé‡
        )
        
        return min(1.0, final_score)

    def is_context_match(self, question: str, answer: str, threshold: float = 0.3) -> bool:
        """åˆ¤æ–­é—®é¢˜å’Œå›ç­”çš„è¯­å¢ƒæ˜¯å¦åŒ¹é…"""
        match_score = self.calculate_context_match(question, answer)
        return match_score >= threshold 